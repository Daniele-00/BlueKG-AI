agent_models:
  coder: gemini-2.5-flash
  contextualizer: gemini-2.5-flash
  entity_extractor: gemini-2.5-flash
  general_conversation: gemini-2.5-flash
  router: gemini-2.5-flash
  social_conversation: gemini-2.5-flash
  social_router: gemini-2.5-flash
  synthesizer: gemini-2.5-flash
  translator: gemini-2.5-flash
available_models:
  gemini-2.5-flash:
    description: Google Gemini 2.5 Flash - 1M token context
    max_tokens: 8000
    model_name: gemini-2.5-flash
    provider: google
  gemini-2.5-pro:
    description: Google Gemini 2.5 Pro - Ottimo per reasoning
    max_tokens: 4000
    model_name: gemini-2.5-pro
    provider: google
  gpt-4o-mini:
    description: GPT-4o Mini - Economico
    max_tokens: 4096
    model_name: gpt-4o-mini
    provider: openai
  gpt4-turbo:
    description: "GPT-4 Turbo - Alta qualit\xE0"
    max_tokens: 4000
    model_name: gpt-4-turbo
    provider: openai
  gpt4o:
    description: GPT-4 Omni - Top quality
    max_tokens: 4096
    model_name: gpt-4o
    provider: openai
  llama-70b-groq-versatile:
    description: Llama 3.3 70B su Groq - LATEST, Best per Cypher
    max_tokens: 8000
    model_name: llama-3.3-70b-versatile
    provider: groq
  llama-8b-groq-instant:
    description: Llama 3.1 8B su Groq - Ultra veloce per traduzioni
    max_tokens: 8000
    model_name: llama-3.1-8b-instant
    provider: groq
  llama3:
    description: Llama 3 locale
    max_tokens: 8096
    model_name: llama3
    provider: ollama
  llama3-70b-groq-legacy:
    description: Llama 3.0 70B su Groq - Legacy stabile
    max_tokens: 8192
    model_name: llama3-70b-8192
    provider: groq
  llama3-8b-groq-legacy:
    description: Llama 3.0 8B su Groq - Legacy veloce
    max_tokens: 8192
    model_name: llama3-8b-8192
    provider: groq
  mistral:
    description: Mistral locale
    max_tokens: 4096
    model_name: mistral
    provider: ollama
providers:
  google:
    api_key_env: GOOGLE_API_KEY
    fallback_model: gemini-2.5-flash
    temperature: 0.5
    timeout: 30
  groq:
    api_key_env: GROQ_API_KEY
    temperature: 0.1
    timeout: 30
  ollama:
    base_url: http://172.16.30.23:11434
    temperature: 0.0
    timeout: 120
  openai:
    api_key_env: OPENAI_API_KEY
    temperature: 0.4
    timeout: 30
  vertex_ai:
    temperature: 0.0
    timeout: 60
